{
  "courses": [
    {
      "id": "comp-arch",
      "name": "Computer Architectures",
      "chapters": [
        {

          "id": "arithmetic",
          "name": "Arithmetic",
          "items": [
            { "q": "What does the ALU do, and what data types can it handle?", "a": "Executes arithmetic/logic; always integers, and floating-point via an on-chip FPU or separate coprocessor." },
            { "q": "Which two signed integer representations are covered?", "a": "Sign-magnitude and two’s complement." },
            { "q": "Why is two’s complement preferred over sign-magnitude?", "a": "Single zero, uniform add/sub logic (no special sign handling)." },
            { "q": "How do you negate a two’s-complement value?", "a": "Bitwise NOT then +1 (to LSB)." },
            { "q": "Give the 8-bit range for two’s complement.", "a": "−128 (1000 0000) to +127 (0111 1111)." },
            { "q": "What happens when negating −128 in 8-bit two’s complement?", "a": "It stays −128 (overflow case); sign should flip but cannot." },
            { "q": "How do you sign-extend when widening?", "a": "Copy the sign bit: pad with 0s if positive, 1s if negative." },
            { "q": "Rule shown on slides for detecting overflow in add/sub?", "a": "Monitor the sign bit — overflow if it changes unexpectedly." },
            { "q": "How is subtraction implemented in hardware?", "a": "a − b = a + (two’s complement of b); only need adder + complementer." },
            { "q": "Why can one adder unit do both add and sub?", "a": "Because subtraction is addition with complemented subtrahend." },
            { "q": "What is the 'schoolbook' unsigned binary multiplication method?", "a": "For each 1 in multiplier → add shifted multiplicand; for 0 → add 0; sum partial products; result is double-length." },
            { "q": "How to handle signed multiplication (simple way)?", "a": "Convert to positive, multiply, negate if signs differ." },
            { "q": "What algorithm handles signed multiplication efficiently?", "a": "Booth’s algorithm." },
            { "q": "Why is Booth’s algorithm useful?", "a": "Reduces additions for runs of 1s in the multiplier (efficient signed multiply)." },
            { "q": "What’s the basis of unsigned binary division?", "a": "Long-division style: shift, subtract, set quotient bits; track remainder." },
            { "q": "Why are signed divisions tricky?", "a": "Extra complexity handling signs and edge cases; algorithm given for unsigned with flowchart." },
            { "q": "Write the general floating-point form used.", "a": "± (significand) × 2^(exponent)." },
            { "q": "What does normalization do?", "a": "Adjust exponent so mantissa MSB is 1; that leading 1 can be implicit (not stored)." },
            { "q": "How are signs/exponents stored in FP (per slides)?", "a": "Mantissa in two’s complement; exponent in excess/bias notation (example: excess-128)." },
            { "q": "What FP range/precision figures are highlighted for 32-bit?", "a": "~±2^256 (≈1.5×10^77) range; ~2^−23 precision ≈ 1.2×10^−7 (~6 decimal digits)." },
            { "q": "High-level FP add/sub procedure?", "a": "Check zeros → align significands (adjust exponents) → add/sub → normalize." },
            { "q": "High-level FP mul/div procedure?", "a": "Check zeros → add/sub exponents → mul/div significands → normalize → round (use double-length intermediates)." },
            { "q": "Which FP standard is referenced?", "a": "IEEE-754 (32/64-bit; extended formats for intermediates)." },
            { "q": "Example of binary fixed-point from slides?", "a": "1001.1010₂ = 9.625₁₀." },
            { "q": "Show partial-product pattern for 1011₂ × 1101₂.", "a": "1011, 0000, 1011, 1011 → product 1000 1111₂ (143₁₀), double-length." },
            { "q": "In unsigned long division, what do partial remainders track?", "a": "The running remainder after each subtract/shift step that drives the next quotient bit." },
            { "q": "Convert −18₁₀ to 8-bit two’s complement.", "a": "+18 = 0001 0010 → invert = 1110 1101 → +1 = 1110 1110." },
            { "q": "Sign-extend 8-bit 1001 0010₂ to 16-bit.", "a": "Negative → pad with 1s: 1111 1111 1001 0010." },
            { "q": "Why is multiply result 'double-length'?", "a": "n-bit × n-bit can require up to 2n bits (sum of shifted partial products)." },
            { "q": "Booth trigger condition (conceptual)?", "a": "Encodes runs of 1s in the multiplier to reduce adds (handle 10/01 transitions efficiently)." },
            { "q": "Core steps for unsigned division?", "a": "Align divisor via shifts → compare/subtract → set quotient bit → iterate → final remainder." },
            { "q": "What does excess-128 mean on an 8-bit exponent?", "a": "Stored exponent = true exponent + 128." },
            { "q": "Why keep intermediate FP results in double length?", "a": "To avoid precision loss before final rounding/normalization." },
            { "q": "First step when adding FP numbers with different exponents?", "a": "Shift the smaller significand to align exponents." }
          ]
        },
        {
        "id": "internal-memory",
        "name": "Internal Memory",
        "items": [
            { "q": "Explain DRAM vs. SRAM in terms of storage mechanism, speed, density, and cost.", "a": "DRAM: stores bits as charge in capacitors, needs periodic refresh, slower, higher density, cheaper. SRAM: stores bits in flip-flops, no refresh, faster, lower density, more expensive." },
            { "q": "Why is DRAM essentially analogue in nature?", "a": "Because it stores charge levels on a capacitor that must be sensed and compared to a reference to determine if it's a 0 or 1." },
            { "q": "Why is DRAM read destructive?", "a": "Reading discharges the capacitor, so the stored data must be rewritten after each read." },
            { "q": "What is the function of the sense amplifier in DRAM?", "a": "Detects tiny voltage differences on the bitline caused by the cell and amplifies them to full logic levels, also restoring the cell’s charge." },
            { "q": "Explain the concept of row and column multiplexing in DRAM.", "a": "DRAM uses the same address pins for row and column addresses; RAS selects the row, CAS selects the column, reducing pin count." },
            { "q": "Why is refresh required for DRAM?", "a": "Charge leaks over time from capacitors; refresh restores charge to prevent data loss." },
            { "q": "Describe the DRAM refresh cycle.", "a": "Each row is read by the sense amplifier (restoring data) and written back before charge decays." },
            { "q": "What is the difference between asynchronous DRAM and SDRAM?", "a": "Asynchronous DRAM is not tied to a clock, accesses occur in fixed sequences; SDRAM is synchronized to the system clock, enabling burst transfers." },
            { "q": "Why is SDRAM faster than conventional DRAM?", "a": "Synchronous operation with CPU clock, pipelined commands, and burst transfers reduce latency per word." },
            { "q": "What is DDR SDRAM and how does it improve performance?", "a": "Double Data Rate SDRAM transfers data on both rising and falling edges of the clock, doubling throughput without increasing clock frequency." },
            { "q": "What is the prefetch architecture in DDR?", "a": "The number of bits fetched internally per read command; DDR2 = 4-bit, DDR3 = 8-bit prefetch." },
            { "q": "What improvement does DDR2 have over DDR?", "a": "Higher speeds, lower voltage (1.8V vs 2.5V), improved signaling and prefetching." },
            { "q": "What improvement does DDR3 have over DDR2?", "a": "Even higher speeds, lower voltage (1.5V), improved power efficiency, larger prefetch." },
            { "q": "What is memory interleaving?", "a": "Splitting memory into multiple banks and accessing them in parallel to increase effective throughput and hide latency." },
            { "q": "What is ECC memory and what can it correct?", "a": "Error-Correcting Code memory detects and corrects single-bit errors and detects (but cannot correct) double-bit errors (SECDED)." },
            { "q": "Difference between hard failure and soft error in memory.", "a": "Hard failure is a permanent defect in the memory cell or circuit. Soft error is a temporary bit flip caused by environmental factors like cosmic rays." },
            { "q": "List ROM types in order of least to most rewritable.", "a": "Mask ROM → PROM → EPROM → EEPROM → Flash." },
            { "q": "How is an EPROM erased?", "a": "Using ultraviolet light through a quartz window to discharge the floating gates." },
            { "q": "How is EEPROM erased?", "a": "Electrically, allowing selective byte erasure and rewriting." },
            { "q": "Why is SRAM used for cache memory?", "a": "Extremely low latency and high-speed random access, no refresh requirement." },
            { "q": "Order the memory hierarchy from fastest to slowest.", "a": "Registers → L1 cache → L2 cache → L3 cache → DRAM → SSD/HDD." },
            { "q": "Difference between NOR and NAND flash.", "a": "NOR: direct random access, good for code execution, more expensive, lower density. NAND: faster for sequential data, higher density, lower cost, requires page/block operations." },
            { "q": "Define a flash memory page and block.", "a": "Page: smallest read/write unit. Block: group of pages, smallest erase unit." },
            { "q": "Why must flash memory be erased before rewriting?", "a": "Because flash stores charge on a floating gate; erasure resets all cells in a block to 1 before programming new data." },
            { "q": "Compare SLC, MLC, and TLC flash.", "a": "SLC: 1 bit/cell, fastest, most durable, most expensive. MLC: 2 bits/cell, balanced cost and performance. TLC: 3 bits/cell, cheapest, lowest endurance and speed." },
            { "q": "What is the TRIM command in SSDs?", "a": "Tells the SSD which data blocks are no longer needed, allowing the controller to erase them in advance for faster future writes." },
            { "q": "What is wear leveling in flash memory?", "a": "A technique to distribute write/erase cycles evenly across all blocks to extend lifespan." },
            { "q": "What is the main storage medium in SSDs?", "a": "NAND flash memory cells." },
            { "q": "What does the SSD controller do?", "a": "Manages mapping logical to physical addresses, wear leveling, garbage collection, error correction, and bad block management." },
            { "q": "What is garbage collection in SSDs?", "a": "The process of consolidating valid data and erasing unused blocks to make space for new writes." },
            { "q": "What is write amplification?", "a": "The ratio of physical writes to host writes; occurs when extra writes are needed due to block erasures and data movement." },
            { "q": "Why does an SSD slow down when nearly full?", "a": "Less free space means more data movement during writes, increasing latency." },
            { "q": "How do SSDs handle bad blocks?", "a": "By marking them as unusable and remapping data to spare blocks using the controller." },
            { "q": "Why are sequential writes generally faster than random writes in SSDs?", "a": "Because they can be written in large continuous chunks, minimizing block erases and data movement." },
            { "q": "How does over-provisioning improve SSD performance?", "a": "It reserves extra flash space for the controller to manage wear leveling and garbage collection more efficiently." }
        ]
        },
        {
        "id": "external-memory",
        "name": "External Memory",
        "items": [
            { "q": "List the three main types of external memory covered in the course.", "a": "Magnetic disk, optical storage (CD/DVD/Blu-ray), and magnetic tape." },
            { "q": "Why is glass now preferred over aluminium for magnetic disk substrates?", "a": "Glass offers improved surface uniformity (reliability), fewer defects (reduced errors), supports lower flying heights, better stiffness, and higher shock resistance." },
            { "q": "Describe the basic read/write process on a magnetic disk.", "a": "Write: current through the coil produces a magnetic field that changes polarity on the disk surface. Read (traditional): moving magnetic field induces current in the coil. Read (modern): separate magnetoresistive head detects changes in resistance due to magnetic field direction." },
            { "q": "What is the difference between CAV and zone bit recording?", "a": "CAV rotates at constant speed, same bits per track (outer tracks waste space). Zone bit recording uses zones with more bits per outer track to increase capacity." },
            { "q": "What are tracks, sectors, and cylinders?", "a": "Tracks: concentric circles on the platter. Sectors: smallest addressable storage unit within a track. Cylinder: all tracks aligned vertically across platters." },
            { "q": "Explain the difference between fixed-head and movable-head disks.", "a": "Fixed-head: one head per track, no head movement. Movable-head: one head per platter side, mounted on an arm that moves between tracks." },
            { "q": "Why use multiple platters in a hard drive?", "a": "To increase capacity and performance; aligned heads form cylinders so data can be read/written across platters with less head movement." },
            { "q": "Define seek time, rotational latency, and access time.", "a": "Seek time: time to position head over correct track. Rotational latency: time waiting for desired sector to rotate under head. Access time = seek + latency." },
            { "q": "What is RAID and why is it used?", "a": "Redundant Array of Independent/Inexpensive Disks; combines multiple physical disks for higher speed, capacity, and/or fault tolerance using striping, mirroring, or parity." },
            { "q": "Summarize RAID 0, 1, 5, and 6.", "a": "RAID 0: striped, no redundancy, max speed. RAID 1: mirrored, high reliability, costlier. RAID 5: striped with distributed parity, balanced performance/reliability. RAID 6: like RAID 5 but with two parity blocks for higher fault tolerance." },
            { "q": "Why is RAID 2 rarely used?", "a": "It uses bit-level striping with multiple dedicated parity disks for Hamming code error correction, making it expensive and overly redundant." },
            { "q": "Explain the principle of optical storage (CD/DVD).", "a": "Data stored as pits and lands on a reflective surface; read by laser detecting changes in reflectivity. Smaller pits and shorter wavelength lasers increase capacity." },
            { "q": "What does 'constant linear velocity' mean in CD drives?", "a": "Disc speed varies so that data passes under the laser at a constant rate, ensuring uniform bit density across the spiral track." },
            { "q": "What’s the capacity difference between DVD and Blu-ray single layers?", "a": "DVD single layer: ~4.7 GB; Blu-ray single layer: ~25 GB." },
            { "q": "Why can random access be slow on CD-ROMs?", "a": "Requires moving the head to approximate position, adjusting rotational speed, and reading addresses before reaching target data." },
            { "q": "What is the main advantage of CD-RW over CD-R?", "a": "CD-RW can be erased and rewritten using phase-change materials, while CD-R is write-once." },
            { "q": "Why did Blu-ray achieve higher capacity than DVD?", "a": "Uses a shorter wavelength blue-violet laser, smaller pits, and a data layer closer to the surface for tighter focus and less distortion." },
            { "q": "What is magnetic tape mainly used for today?", "a": "Backup and archival storage due to low cost per bit and high capacity, despite slow access times." },
            { "q": "Explain 'serial access' in the context of magnetic tape.", "a": "Data can only be read sequentially from the start to the target location, unlike random access on disks." },
            { "q": "What is LTO and why is it important?", "a": "Linear Tape-Open: an open-standard high-capacity magnetic tape format for enterprise backup, evolving through multiple generations with increasing capacity and transfer rates." }
        ]
        },
        {
          "id": "instruction-cycles",
          "name": "Instruction Cycles",
          "items": [
            { "q": "Define the instruction cycle at a high level.", "a": "The recurring sequence the CPU performs for each instruction: Fetch (get the next instruction), Decode (interpret opcode/operands), Execute (perform the action), plus an optional Interrupt cycle to service pending interrupts between instructions." },
            { "q": "List the registers most directly involved in the fetch cycle and their roles.", "a": "PC (Program Counter): holds address of next instruction. MAR (Memory Address Register): receives the address to access memory. MBR/MDR (Memory Buffer/Data Register): holds data read from or written to memory. IR (Instruction Register): latches the fetched instruction for decode/execute." },
            { "q": "Walk through the standard fetch sequence as micro-operations.", "a": "1) MAR ← PC; 2) Memory read; 3) IR ← MBR; 4) PC ← PC + 1 (or + instruction length). Now the instruction in IR is ready for decoding." },
            { "q": "What happens during the decode phase?", "a": "The control unit interprets the IR’s opcode/format, determines required operand sources/destinations and addressing mode(s), and generates the control signals for the execute phase." },
            { "q": "Name the four broad classes of actions in the execute cycle.", "a": "Processor–memory (load/store), Processor–I/O (I/O read/write), Data processing (ALU operations), and Control (alter sequence via branch/jump/call/return)." },
            { "q": "Explain how a branch instruction alters the basic fetch–execute flow.", "a": "If the branch condition is met, the CPU loads the PC with a new target address (computed from immediate or register/ALU result) instead of the next sequential PC, changing the next fetch location." },
            { "q": "What is an interrupt and why is it used?", "a": "An asynchronous signal/event that diverts normal execution so the CPU can handle conditions like I/O completion, timer ticks, program exceptions, or hardware faults—improving responsiveness and overlap of I/O with computation." },
            { "q": "List common interrupt sources.", "a": "Program (e.g., divide by zero, overflow, illegal opcode), Timer (periodic tick), I/O (device signals completion or service request), and Hardware failure (e.g., parity error)." },
            { "q": "Outline the steps of the interrupt cycle.", "a": "1) CPU checks for pending/enable interrupts; 2) if one is pending, current context is saved (PC, status, key registers); 3) PC is loaded with the interrupt handler’s entry address (via vector or fixed entry); 4) handler executes; 5) context is restored and normal program resumes." },
            { "q": "What is the role of the PSW/flags during an interrupt?", "a": "Interrupt enable/priority bits are examined; status flags and the current mode are saved so the handler can run with appropriate privileges and the original state can be restored." },
            { "q": "Contrast vectored vs. non-vectored interrupts.", "a": "Vectored: the device/priority provides an interrupt vector (address) to jump directly to its specific handler. Non-vectored: the CPU branches to a fixed location; software must poll status to identify the source." },
            { "q": "Explain interrupt masking.", "a": "Software or hardware can disable (mask) certain interrupts to protect critical sections; masked requests remain pending until re-enabled." },
            { "q": "Compare sequential vs nested interrupt handling.", "a": "Sequential: CPU disables further interrupts while servicing one; pending ones are handled afterward. Nested: allows higher-priority interrupts to preempt a lower-priority handler; after servicing, control returns to the preempted handler." },
            { "q": "Why do interrupts improve overall system performance with slow I/O?", "a": "They let the CPU continue executing other tasks while devices operate; when the device finishes, it interrupts the CPU instead of the CPU busy-waiting or polling continuously." },
            { "q": "Define access time for memory and how it affects instruction timing.", "a": "Access time is the latency to complete a memory read/write. Fetch and load/store operations are gated by this latency; long memory or I/O waits stretch the overall instruction cycle unless hidden by caches or overlapped by interrupts/DMA." },
            { "q": "Give two timing scenarios that illustrate the effect of I/O latency on a program.", "a": "Short I/O wait: brief stall before execution continues. Long I/O wait: significant idle time unless overlapped by other work or serviced via interrupts/DMA." },
            { "q": "What state information must be saved on an interrupt and why?", "a": "At minimum: PC (return point), status/flags, and any registers the handler will use. This preserves the interrupted program’s exact state so it resumes correctly after the handler completes." },
            { "q": "Explain the difference between exceptions (synchronous) and interrupts (asynchronous).", "a": "Exceptions are triggered by the current instruction’s execution (e.g., page fault, divide-by-zero); interrupts originate externally and can occur between instructions regardless of the current instruction semantics." },
            { "q": "Describe the top-level CPU–Memory–I/O interconnections relevant to the cycle.", "a": "CPU issues addresses and control signals over buses; memory and I/O modules respond with data or status. The instruction cycle repeatedly uses these interconnects: fetch uses CPU→Memory read; execute may use CPU↔Memory or CPU↔I/O transfers." },
            { "q": "How does the Program Counter (PC) get updated across different instruction types?", "a": "Normally PC ← PC + instruction length at the end of fetch; branches/jumps/calls overwrite PC with a target; returns restore PC from a saved value (e.g., stack/link register)." },
            { "q": "Where do operands live from the CPU’s perspective?", "a": "In registers (fastest), in main memory, or in I/O device buffers. The instruction’s addressing mode indicates where to fetch them from and where to place results." },
            { "q": "Give the micro-operations for a typical load instruction.", "a": "1) MAR ← EffectiveAddress; 2) Memory read; 3) Register_dest ← MBR." },
            { "q": "Give the micro-operations for a typical store instruction.", "a": "1) MAR ← EffectiveAddress; 2) MBR ← Register_src; 3) Memory write." },
            { "q": "Give the micro-operations for a typical ALU instruction (register-register).", "a": "1) ALU_out ← f(Rs, Rt); 2) Rd ← ALU_out; 3) Update flags as needed; PC already advanced in fetch." },
            { "q": "How do addressing modes relate to the instruction cycle?", "a": "During decode/execute, the CPU computes the Effective Address (EA) based on the addressing mode (e.g., immediate, direct, indirect, register, displacement/PC-relative), then performs the operand access via memory or registers." },
            { "q": "Explain PC-relative addressing in the context of the cycle.", "a": "The Effective Address is computed as EA = PC + displacement (often using the PC value after fetch). This supports position-independent code and good locality for branches and data near the code." },
            { "q": "How does stack use appear in the instruction cycle for subroutine calls/returns?", "a": "Call: push return PC (and possibly PSW/registers), then set PC to subroutine address. Return: pop saved PC (and state) to resume execution exactly after the call." },
            { "q": "What is the Instruction Register (IR) and why is it necessary?", "a": "IR holds the current instruction while it’s being decoded and executed, stabilizing the opcode/fields independent of memory timing and allowing the control unit to generate signals." },
            { "q": "Why is an instruction cycle state diagram useful?", "a": "It formalizes states (Fetch, Indirect/EA compute if needed, Execute, Interrupt check/serve) and the transitions between them, clarifying control sequencing and where stalls or interrupts can occur." },
            { "q": "How does the CPU decide when to check for interrupts?", "a": "Typically at well-defined points—most commonly after finishing an instruction (before starting the next fetch). Some architectures also check during specific long operations or at micro-state boundaries." },
            { "q": "What is the effect of disabling interrupts globally?", "a": "All maskable interrupts are deferred; they remain pending until interrupts are re-enabled, ensuring critical sections run atomically but reducing responsiveness." },
            { "q": "What determines the order in which multiple pending interrupts are handled?", "a": "Priority schemes (fixed or programmable). Higher-priority interrupts may either preempt lower-priority ones (nested handling) or be serviced first after the current handler completes (sequential handling)." },
            { "q": "Explain the relationship between the instruction cycle and bus activity.", "a": "Fetch and operand accesses assert addresses and control signals on the address/control buses; data moves on the data bus. The cadence of the instruction cycle determines bus utilization patterns." },
            { "q": "Why do complex instructions (e.g., with memory operands) lengthen the execute phase?", "a": "They may require extra memory cycles (to compute EA, to fetch/store operands) and additional control sequencing compared to simple register-to-register ALU ops." },
            { "q": "How do caches influence the instruction cycle timing?", "a": "Cache hits shorten fetch/load latency dramatically; misses incur main-memory access and stall the cycle until data returns (unless overlapped by out-of-order or prefetching in more advanced CPUs)." },
            { "q": "What is the difference between polling and interrupt-driven I/O in the cycle?", "a": "Polling: CPU repeatedly checks device status during execute, wasting cycles when idle. Interrupt-driven: CPU proceeds with other work; device triggers an interrupt when ready, leading into the interrupt cycle." },
            { "q": "Show the fetch-decode-execute-interrupt loop in pseudocode form.", "a": "loop { IR ← Mem[PC]; PC ← PC + L; decode(IR); execute(IR); if (INT_pending & enabled) { save_context(); PC ← handler_addr; run_handler(); restore_context(); } }" },
            { "q": "Why must the return address be saved before servicing an interrupt?", "a": "So that normal execution can resume at the exact instruction boundary where it was interrupted, preserving program correctness and flow." },
            { "q": "What is the role of the control unit in the instruction cycle?", "a": "It sequences micro-operations, issues control signals to datapath and buses, handles branching decisions, and orchestrates interrupt acknowledgment and context save/restore." },
            { "q": "Explain how DMA interacts with the instruction cycle.", "a": "DMA lets an I/O controller transfer blocks between memory and device without CPU involvement; the CPU’s instruction cycle is momentarily stalled only for bus arbitration, while the transfer proceeds in parallel." },
            { "q": "Why do architectures define privilege levels in relation to interrupts?", "a": "Interrupt handlers often need to access protected resources; entering a privileged mode on interrupt ensures safe access, while user mode prevents applications from executing sensitive operations." },
            { "q": "Differentiate between synchronous control flow changes (branches/calls) and asynchronous ones (interrupts).", "a": "Synchronous changes are dictated by the executing instruction stream; asynchronous changes are triggered by external events and occur between instructions (architecturally)." },
            { "q": "What happens if an interrupt arrives during the fetch of the next instruction?", "a": "Architecturally, most CPUs complete the current instruction (including its fetch) before acknowledging the interrupt; then they enter the interrupt cycle and branch to the handler." },
            { "q": "Give an example of micro-ops for a conditional branch that is taken.", "a": "If condition flag set: PC ← PC + displacement (or PC ← target_reg/ALU result); else PC remains sequential. Control unit sets these based on flags evaluated during execute." },
            { "q": "Why do some architectures include an 'indirect' stage in the instruction cycle?", "a": "If the addressing mode is indirect, an extra memory read is needed to fetch the pointer to the operand before the actual operand access—this can be modeled as an 'indirect' sub-cycle between fetch and execute." },
            { "q": "Summarize the main benefit of structuring execution into well-defined cycles.", "a": "It cleanly separates concerns (fetch vs. execute vs. interrupt handling), simplifies control sequencing and hardware design, and provides clear points for performance optimizations and precise exception handling." }
          ]
        },
        {
          "id": "processor-structure-parallelism",
          "name": "Processor Structure and Parallelism",
          "items": [
            { "q": "What are the five main tasks of a CPU?", "a": "1) Fetch instruction from memory, 2) Decode/interpret instruction, 3) Fetch operands if needed, 4) Execute the instruction, 5) Store results." },
            { "q": "What role do registers play in processor performance?", "a": "Registers are the fastest form of storage, directly accessible by the CPU. They reduce memory access, support instruction execution, and their number and type affect CPU speed." },
            { "q": "Differentiate between general-purpose, data, address, and condition-code registers.", "a": "General-purpose: flexible use for data/addresses. Data registers: hold operands. Address registers: hold memory locations. Condition-code (flags): hold status bits (zero, carry, sign, overflow)." },
            { "q": "What is the Program Status Word (PSW)?", "a": "A collection of bits reflecting CPU state: condition codes (zero, carry, overflow), interrupt enables, execution mode (user/supervisor), and other control bits." },
            { "q": "Why are user mode and supervisor mode necessary?", "a": "User mode restricts execution to non-privileged instructions to protect the OS and hardware. Supervisor mode allows privileged operations like I/O control, memory management, and interrupt handling." },
            { "q": "What are the stages of instruction pipelining?", "a": "Common stages: 1) Fetch, 2) Decode, 3) Execute, 4) Memory access, 5) Write-back. Not all processors use all five, but the idea is to overlap execution." },
            { "q": "What is the main goal of pipelining?", "a": "To increase instruction throughput by overlapping stages of multiple instructions, so multiple instructions are processed simultaneously." },
            { "q": "What are pipeline hazards?", "a": "Situations that stall or disrupt pipeline flow: 1) Structural hazards (resource conflicts), 2) Data hazards (instruction depends on previous result), 3) Control hazards (branch/jump instructions)." },
            { "q": "Give an example of a structural hazard.", "a": "If two pipeline stages need the same memory port in the same cycle, the hardware cannot serve both, causing a stall." },
            { "q": "Give an example of a data hazard.", "a": "Instruction 2 uses the result of Instruction 1 before it has been written back. Example: I1: R1 ← R2 + R3; I2: R4 ← R1 + R5. I2 depends on I1’s result." },
            { "q": "Give an example of a control hazard.", "a": "A conditional branch whose target is not known until after execution. The pipeline may fetch wrong instructions and need to flush them." },
            { "q": "How are data hazards solved?", "a": "1) Forwarding/bypassing (send result directly to dependent instruction), 2) Pipeline stalling (insert bubbles), 3) Compiler scheduling (reorder instructions)." },
            { "q": "How are control hazards solved?", "a": "Techniques: branch prediction (static/dynamic), delayed branching, or pipeline flushing with penalties." },
            { "q": "What is branch prediction?", "a": "A mechanism to guess branch outcomes before resolved. Static prediction uses fixed rules, dynamic prediction uses history. Correct guesses improve performance; mispredictions cause stalls." },
            { "q": "What is a superscalar processor?", "a": "A CPU with multiple execution units that can issue and complete multiple instructions per cycle. Requires ILP, complex scheduling, and hazard detection." },
            { "q": "Explain Instruction-Level Parallelism (ILP).", "a": "ILP is executing independent instructions in parallel within a single core. Exploited by pipelining, superscalar execution, out-of-order execution, and speculation." },
            { "q": "What is out-of-order execution?", "a": "A CPU technique where instructions are executed as soon as operands are available, not strictly in program order, to reduce stalls and maximize ILP." },
            { "q": "What is register renaming and why is it needed?", "a": "A technique to remove false data dependencies (name dependencies) by mapping logical registers to physical ones, allowing more parallel execution." },
            { "q": "Differentiate ILP and TLP.", "a": "ILP: parallelism within a single instruction stream (pipelining, superscalar). TLP: parallelism across multiple threads or processes (multicore, SMT)." },
            { "q": "Summarize Flynn’s taxonomy of computer architectures.", "a": "SISD: single instruction, single data. SIMD: single instruction, multiple data (vector processors, GPUs). MISD: multiple instruction, single data (rare). MIMD: multiple instruction, multiple data (multicore, clusters)." },
            { "q": "Give an example of SIMD parallelism in practice.", "a": "Graphics processing (GPU shaders) where the same instruction is applied to many pixels or data elements in parallel." },
            { "q": "What does Amdahl’s Law state?", "a": "Speedup with p processors = 1 / (f + (1-f)/p), where f is the fraction of serial code. Shows diminishing returns as serial portion dominates." },
            { "q": "If 20% of a program is serial, what is the maximum speedup possible according to Amdahl’s Law?", "a": "Maximum speedup = 1 / 0.2 = 5, even with infinite processors." },
            { "q": "What are the implications of Amdahl’s Law for parallel system design?", "a": "Improving the serial portion is critical; adding more processors only helps if most of the workload is parallelizable." },
            { "q": "What is thread-level parallelism (TLP)?", "a": "Executing multiple independent threads or processes simultaneously, either on multiple cores or interleaved on one core (SMT/hyperthreading)." },
            { "q": "How does multicore architecture exploit TLP?", "a": "By placing multiple independent cores on one chip, allowing true MIMD execution of multiple threads, improving throughput for parallel workloads." },
            { "q": "What is simultaneous multithreading (SMT)?", "a": "Technique where a single core executes multiple threads in the same cycle by sharing functional units, improving utilization of idle resources." },
            { "q": "Differentiate multicore vs. multiprocessor systems.", "a": "Multicore: multiple cores on one chip. Multiprocessor: multiple CPUs (sockets) in one system. Both support MIMD but differ in integration and communication latency." },
            { "q": "What is the difference between strong and weak scaling?", "a": "Strong scaling: fixed problem size, add processors to reduce runtime. Weak scaling: increase problem size proportionally to number of processors." }
          ]
        },
        {
          "id": "cache-memory",
          "name": "Cache Memory",
          "items": [
            { "q": "What is cache memory and why is it important?", "a": "Cache is a small, very fast memory between CPU and main memory. It reduces the average memory access time by storing recently and frequently accessed instructions and data. It exploits locality of reference and allows the CPU to run at near its maximum speed without waiting for slower main memory." },
            { "q": "Explain temporal and spatial locality.", "a": "Temporal locality: recently accessed data is likely to be accessed again soon. Spatial locality: data located near recently accessed addresses is likely to be used soon. Caches exploit both by storing recently used blocks and fetching contiguous memory blocks." },
            { "q": "Define cache hit and cache miss.", "a": "Cache hit: the CPU finds the requested word in cache, leading to a fast access. Cache miss: the word is not in cache and must be fetched from main memory, which is slower." },
            { "q": "What is hit ratio and miss ratio?", "a": "Hit ratio = (cache hits ÷ total accesses). Miss ratio = 1 − hit ratio. High hit ratio means most requests are served quickly from cache, reducing average access time." },
            { "q": "Write the formula for average memory access time (AMAT).", "a": "AMAT = Hit time + (Miss ratio × Miss penalty). For multi-level caches, Miss penalty includes the access time of the next level." },
            { "q": "What is a cache block (line)?", "a": "The unit of data exchanged between main memory and cache. A block contains several consecutive words. This reduces miss rate by exploiting spatial locality." },
            { "q": "Why aren’t larger blocks always better?", "a": "Larger blocks increase miss penalty (more data per transfer), may waste cache space if not all words are used, and can evict other useful blocks, increasing conflict misses." },

            { "q": "What are the three main mapping techniques?", "a": "Direct mapping, fully associative mapping, and set-associative mapping." },
            { "q": "How does direct mapping work?", "a": "Each memory block maps to exactly one cache line (index = block address mod number of lines). It’s simple and fast, but multiple blocks competing for the same line cause conflict misses." },
            { "q": "How does fully associative mapping work?", "a": "Any memory block can be placed in any cache line. Flexible and avoids conflict misses, but requires associative (parallel) search hardware, which is costly and slower." },
            { "q": "How does set-associative mapping work?", "a": "Cache is divided into sets, each with multiple lines (ways). A memory block maps to exactly one set but can be stored in any line of that set. It balances speed and flexibility." },
            { "q": "What does 'n-way set associative' mean?", "a": "Each set has n lines. Example: 4-way set associative means each set has 4 possible places to store a block." },

            { "q": "What are cache replacement policies?", "a": "When a set is full and a new block must be placed, a line must be replaced. Common policies: Least Recently Used (LRU), First-In-First-Out (FIFO), and Random." },
            { "q": "Explain LRU, FIFO, and Random replacement.", "a": "LRU replaces the block that hasn’t been used for the longest time (best performance but hardware-expensive). FIFO replaces the oldest block loaded. Random replaces a randomly chosen block, simple but less optimal." },

            { "q": "What are the two main write policies in caches?", "a": "Write-through and write-back." },
            { "q": "Explain write-through cache.", "a": "Every write updates both cache and main memory. Simple but generates more memory traffic. Often combined with a write buffer to reduce stalls." },
            { "q": "Explain write-back cache.", "a": "Write updates only the cache line and marks it dirty. The main memory is updated only when the dirty block is replaced. Reduces memory traffic but adds complexity." },
            { "q": "What is write-allocate vs no-write-allocate?", "a": "Write-allocate: on a write miss, the block is loaded into cache, then written. No-write-allocate: on a write miss, data is written directly to memory without loading into cache." },

            { "q": "Why do modern CPUs use multi-level caches?", "a": "Because a single large fast cache is impractical. L1 cache is very small and fast, L2 larger but slower, and L3 even larger and often shared. This hierarchy balances access speed and storage capacity." },
            { "q": "What are inclusive and exclusive cache policies?", "a": "Inclusive: all data in L1 is guaranteed to be in L2/L3, simplifying coherence. Exclusive: a block appears in only one level, maximizing effective capacity. Non-inclusive/non-exclusive is also used." },
            { "q": "Give an example of cache hierarchy in a real CPU.", "a": "A modern Intel Core CPU may have 32 KB L1 instruction cache and 32 KB L1 data cache (8-way set associative), a 256 KB private L2 cache, and a shared multi-MB L3 cache." },

            { "q": "What are the main types of cache misses?", "a": "Compulsory misses (first access to a block), conflict misses (due to mapping conflicts), and capacity misses (cache too small to hold working set)." },
            { "q": "Why can conflict misses occur even if cache has free space?", "a": "In direct-mapped or limited set-associative caches, multiple blocks map to the same line or set, forcing replacements." },
            { "q": "How can misses be reduced?", "a": "Increase block size (reduces compulsory misses), increase associativity (reduces conflict misses), or increase cache size (reduces capacity misses)." },

            { "q": "What is the difference between unified and split caches?", "a": "Unified cache stores both instructions and data. Split cache has separate instruction and data caches (Harvard architecture style). Split caches allow parallel instruction/data access but may be less flexible." },

            { "q": "How do caches impact CPU performance?", "a": "They reduce the average memory access time dramatically. Without caches, CPUs would stall frequently waiting for slow main memory. With a high hit ratio, effective CPU speed approaches cache speed." },
            { "q": "Explain the trade-off between hit time and miss rate.", "a": "Smaller caches are faster (low hit time) but have higher miss rates. Larger caches reduce misses but have slower hit time. Designers balance these trade-offs for optimal performance." }
          ]
        }

      ]
    }
  ]
}
